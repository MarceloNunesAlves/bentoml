{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao BentoML\n",
    "\n",
    "[BentoML](http://bentoml.ai) é uma estrutura de código aberto para **servir modelos de ML**, com o objetivo de preencher a **lacuna entre Data Science e DevOps**.\n",
    "\n",
    "Os cientistas de dados podem facilmente empacotar seus modelos treinados com qualquer estrutura de ML usando BentoMl e reproduzir o modelo para servir na produção. O BentoML ajuda no gerenciamento de modelos empacotados no formato BentoML e permite que o DevOps os implante como APIs on-line servindo endpoints ou trabalhos de inferência em lote off-line, em qualquer plataforma de nuvem.\n",
    "\n",
    "Este guia de introdução demonstra como usar BentoML para servir a um sklearn modeld por meio de um servidor REST API e, a seguir, colocar o servidor de modelo em contêiner para implantação de produção.\n",
    "\n",
    "BentoML requer python 3.6 ou superior, instale dependências via `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyPI packages required in this guide, including BentoML\n",
    "!pip install -q --pre bentoml  # install preview version of BentoML for this guide\n",
    "!pip install -q 'scikit-learn>=0.23.2' 'pandas>=1.1.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de começar, vamos discutir como seria a estrutura do projeto do BentoML. Para a maioria dos casos de uso, os usuários podem seguir esta estrutura mínima\n",
    "para implantar com BentoML para evitar possíveis erros (exemplo de estrutura de projeto pode ser encontrado em [guides/quick-start](https://github.com/bentoml/BentoML/tree/master/guides/quick-start)):\n",
    "\n",
    "    bento_deploy/\n",
    "    ├── bento_packer.py        # responsible for packing BentoService\n",
    "    ├── bento_service.py       # BentoService definition\n",
    "    ├── model.py               # DL Model definitions\n",
    "    ├── train.py               # training scripts\n",
    "    └── requirements.txt\n",
    "\n",
    "Vamos preparar um modelo treinado para servir com BentoML. Treine um modelo classificador no [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crie um serviço de previsão com BentoML\n",
    "\n",
    "O serviço de modelo com BentoML vem depois que um modelo é treinado. A primeira etapa é criar um\n",
    "classe de serviço de previsão, que define os modelos necessários e as APIs de inferência que\n",
    "contém a lógica de serviço. Aqui está um serviço de previsão mínimo criado para servir\n",
    "o modelo do classificador da íris treinado acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bento_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bento_service.py\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "\n",
    "@env(infer_pip_packages=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "    \"\"\"\n",
    "    A minimum prediction service exposing a Scikit-learn model\n",
    "    \"\"\"\n",
    "\n",
    "    @api(input=DataframeInput(), batch=True)\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        An inference API named `predict` with Dataframe input adapter, which codifies\n",
    "        how HTTP requests or CSV files are converted to a pandas Dataframe object as the\n",
    "        inference API function input\n",
    "        \"\"\"\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código define um serviço de previsão que empacota um modelo scikit-learn e fornece uma API de inferência que espera um objeto `pandas.Dataframe` como sua entrada. BentoML também suporta outras entradas de API do tipos de dados, incluindo `JsonInput`,` ImageInput`, `FileInput` e [mais] (https://docs.bentoml.org/en/latest/api/adapters.html).\n",
    "\n",
    "Em BentoML, **todas as APIs de inferência devem aceitar uma lista de entradas e retornar um lista de resultados**. No caso de `DataframeInput`, cada linha do dataframe está mapeando a uma solicitação de previsão recebida do cliente. BentoML irá converter HTTP JSON solicitações para o objeto: code: `pandas.DataFrame` antes de passá-lo para o objeto definido pelo usuário função API de inferência.\n",
    "\n",
    "Este design permite que o BentoML agrupe as solicitações de API em pequenos lotes enquanto atende online tráfego. Comparando com um frasco regular ou servidor de modelo baseado em FastAPI, isso pode aumentar a taxa de transferência geral do servidor API em 10-100x, dependendo da carga de trabalho.\n",
    "\n",
    "O código a seguir empacota o modelo treinado com a classe de serviço de previsão `IrisClassifier` definido acima e, em seguida, salva a instância IrisClassifier no disco no formato BentoML para distribuição e implantação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bento_service.IrisClassifier at 0x7f0dec5a03a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the IrisClassifier class defined above\n",
    "from bento_service import IrisClassifier\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3\n",
       "23   5.1  3.3  1.7  0.5\n",
       "149  5.9  3.0  5.1  1.8\n",
       "109  7.2  3.6  6.1  2.5\n",
       "39   5.1  3.4  1.5  0.2\n",
       "22   4.6  3.6  1.0  0.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare input data for testing the prediction service\n",
    "import pandas as pd\n",
    "test_input_df = pd.DataFrame(X).sample(n=5)\n",
    "test_input_df.to_csv(\"./test_input.csv\", index=False)\n",
    "test_input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the service's inference API python interface\n",
    "iris_classifier_service.predict(test_input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-08-03 16:06:42,835] INFO - BentoService bundle 'IrisClassifier:20210803160641_3F5B15' created at: /tmp/tmp2wpondqq\n",
      "[2021-08-03 16:06:42,840] INFO - ======= starting dev server on port: 5000 =======\n",
      "[2021-08-03 16:06:43,305] INFO - Starting BentoML API proxy in development mode..\n",
      "[2021-08-03 16:06:43,307] INFO - Starting BentoML API server in development mode..\n",
      "[2021-08-03 16:06:43,566] INFO - Micro batch enabled for API `predict` max-latency: 20000 max-batch-size 4000\n",
      "[2021-08-03 16:06:43,566] INFO - Your system nofile limit is 4096, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      " * Serving Flask app \"IrisClassifier\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n",
      "[2021-08-03 16:06:58,455] INFO - {'service_name': 'IrisClassifier', 'service_version': '20210803160641_3F5B15', 'api': 'predict', 'task': {'data': '[[5.1, 3.3, 1.7, 0.5], [5.9, 3.0, 5.1, 1.8], [7.2, 3.6, 6.1, 2.5], [5.1, 3.4, 1.5, 0.2], [4.6, 3.6, 1.0, 0.2]]', 'task_id': '930b7994-80e3-4dcd-b71c-0a509e65b308', 'batch': 5, 'http_headers': (('Host', '127.0.0.1:5000'), ('User-Agent', 'python-requests/2.24.0'), ('Accept-Encoding', 'gzip, deflate'), ('Accept', '*/*'), ('Connection', 'keep-alive'), ('Content-Length', '110'), ('Content-Type', 'application/json'))}, 'result': {'data': '[0, 2, 2, 0, 0]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '930b7994-80e3-4dcd-b71c-0a509e65b308'}\n"
     ]
    }
   ],
   "source": [
    "# Start a dev model server to test out everything\n",
    "iris_classifier_service.start_dev_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:5000/predict\",\n",
    "    json=test_input_df.values.tolist()\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-08-03 16:07:09,034] INFO - Dev server has stopped.\n"
     ]
    }
   ],
   "source": [
    "# Stop the dev model server\n",
    "iris_classifier_service.stop_dev_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-08-03 16:07:14,832] INFO - BentoService bundle 'IrisClassifier:20210803160641_3F5B15' saved to: /home/navantb/bentoml/repository/IrisClassifier/20210803160641_3F5B15\n"
     ]
    }
   ],
   "source": [
    "# Save the prediction service to disk for deployment\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML armazena todos os arquivos de modelo empacotados sob o diretório `~ / bentoml / {service_name} / {service_version}` por padrão. O formato de arquivo BentoML contém todos os códigos, arquivos e configurações necessários para implantar o modelo para servir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API Model Serving\n",
    "\n",
    "Para iniciar um servidor de modelo REST API com o `IrisClassifier` salvo acima, use\n",
    "o comando `bentoml serve`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-08-03 16:07:58,185] INFO - Getting latest version IrisClassifier:20210803160641_3F5B15\n",
      "[2021-08-03 16:07:58,195] INFO - Starting BentoML API proxy in development mode..\n",
      "[2021-08-03 16:07:58,196] INFO - Starting BentoML API server in development mode..\n",
      "[2021-08-03 16:07:58,285] INFO - Micro batch enabled for API `predict` max-latency: 20000 max-batch-size 4000\n",
      "[2021-08-03 16:07:58,285] INFO - Your system nofile limit is 4096, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "======== Running on http://0.0.0.0:5000 ========\n",
      "(Press CTRL+C to quit)\n",
      " * Serving Flask app \"IrisClassifier\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:52415/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [03/Aug/2021 16:08:31] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2021 16:08:31] \"\u001b[37mGET /static_content/main.css HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2021 16:08:31] \"\u001b[37mGET /static_content/readme.css HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2021 16:08:31] \"\u001b[37mGET /static_content/swagger-ui.css HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2021 16:08:31] \"\u001b[37mGET /static_content/marked.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2021 16:08:31] \"\u001b[37mGET /static_content/swagger-ui-bundle.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2021 16:08:31] \"\u001b[37mGET /docs.json HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2021 16:08:31] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "[2021-08-03 16:09:24,454] ERROR - Error caught in API function:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/navantb/anaconda3/lib/python3.8/site-packages/bentoml/service/inference_api.py\", line 176, in wrapped_func\n",
      "    return self._user_func(*args, **kwargs)\n",
      "  File \"/home/navantb/bentoml/repository/IrisClassifier/20210803160641_3F5B15/IrisClassifier/bento_service.py\", line 21, in predict\n",
      "    return self.artifacts.model.predict(df)\n",
      "  File \"/home/navantb/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 615, in predict\n",
      "    y = super().predict(X)\n",
      "  File \"/home/navantb/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 333, in predict\n",
      "    X = self._validate_for_predict(X)\n",
      "  File \"/home/navantb/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 465, in _validate_for_predict\n",
      "    X = check_array(X, accept_sparse='csr', dtype=np.float64,\n",
      "  File \"/home/navantb/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/navantb/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 612, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Expected 2D array, got scalar array instead:\n",
      "array=nan.\n",
      "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
      "127.0.0.1 - - [03/Aug/2021 16:09:24] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "[2021-08-03 16:09:58,782] INFO - {'service_name': 'IrisClassifier', 'service_version': '20210803160641_3F5B15', 'api': 'predict', 'task': {'data': '[[5.1,3.5,1.4,0.2]]', 'task_id': 'db8d933b-d4ee-4bcb-a152-0be57e20ce80', 'batch': 1, 'http_headers': (('Host', 'localhost:5000'), ('Connection', 'keep-alive'), ('Content-Length', '19'), ('sec-ch-ua', '\" Not;A Brand\";v=\"99\", \"Google Chrome\";v=\"91\", \"Chromium\";v=\"91\"'), ('accept', '*/*'), ('sec-ch-ua-mobile', '?0'), ('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'), ('Content-Type', 'application/json'), ('Origin', 'http://localhost:5000'), ('Sec-Fetch-Site', 'same-origin'), ('Sec-Fetch-Mode', 'cors'), ('Sec-Fetch-Dest', 'empty'), ('Referer', 'http://localhost:5000/'), ('Accept-Encoding', 'gzip, deflate, br'), ('Accept-Language', 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7'), ('Cookie', 'username-localhost-8888=\"2|1:0|10:1628017410|23:username-localhost-8888|44:M2UzOTk0OTc0ODAxNGM2YzkxZmI1OGZhZmNmMjc4OTc=|3b4f8c81dbf1531530b3f6f1c44e446dbeeb271979d97d1efe3d9718b784d952\"; _xsrf=2|1997ea68|f3e8637e19d9f97ff0f0b1c3bc1747a8|1628017410'))}, 'result': {'data': '[0]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': 'db8d933b-d4ee-4bcb-a152-0be57e20ce80'}\n",
      "127.0.0.1 - - [03/Aug/2021 16:09:58] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve IrisClassifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você estiver executando este notebook do Google Colab, você pode iniciar o servidor de desenvolvimento com a opção `--run-with-ngrok`, para obter acesso ao endpoint da API através de um endpoint público gerenciado por [ngrok](https://ngrok.com/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bentoml serve IrisClassifier:latest --run-with-ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo `IrisClassifier` está no ar no endereço `localhost:5000`. Use o comando `curl` para enviar uma requisição de previsão:\n",
    "\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "Ou com `python` e [request library](https://requests.readthedocs.io/):\n",
    "```python\n",
    "import requests\n",
    "response = requests.post(\"http://127.0.0.1:5000/predict\", json=[[5.1, 3.5, 1.4, 0.2]])\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "Observe que o servidor BentoML API converte automaticamente o formato Dataframe JSON em um objeto `pandas.DataFrame` antes de enviá-lo para a função API de inferência definida pelo usuário.\n",
    "\n",
    "O servidor BentoML API também fornece um painel de interface de usuário da web simples. Vá para http://localhost:5000 no navegador e use a IU da Web para enviar pedido de previsão:\n",
    "\n",
    "![Captura de tela da IU da Web do BentoML API Server](https://raw.githubusercontent.com/bentoml/BentoML/master/guides/quick-start/bento-api-server-web-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Servidor os modelo com contêineres Docker\n",
    "\n",
    "Uma maneira comum de distribuir este modelo de servidor API para implantação de produção é via Recipientes Docker. E o BentoML oferece uma maneira conveniente de fazer isso.\n",
    "\n",
    "Observe que `docker` __não está disponível no Google Colab__. Você precisará baixar e executar este bloco de notas localmente para experimentar este recurso de contêiner com docker.\n",
    "\n",
    "Se você já tiver o docker configurado, basta executar o comando a seguir para produzir um contêiner docker que atende o serviço de previsão `IrisClassifier` criado acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-19 02:37:37,423] INFO - Getting latest version IrisClassifier:20210319023551_84AAF6\n",
      "\u001b[39mFound Bento: /Users/chaoyu/bentoml/repository/IrisClassifier/20210319023551_84AAF6\u001b[0m\n",
      "Containerizing IrisClassifier:20210319023551_84AAF6 with local YataiService and docker daemon from local environment-\u001b[32mBuild container image: iris-classifier:v1\u001b[0m\n",
      "\b \r"
     ]
    }
   ],
   "source": [
    "!bentoml containerize IrisClassifier:latest -t iris-classifier:v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicie um contêiner com a imagem do docker criada na etapa anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-19 09:37:42,276] INFO - Starting BentoML API server in production mode..\n",
      "[2021-03-19 09:37:42 +0000] [1] [INFO] Starting gunicorn 20.0.4\n",
      "[2021-03-19 09:37:42 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n",
      "[2021-03-19 09:37:42 +0000] [1] [INFO] Using worker: sync\n",
      "[2021-03-19 09:37:42 +0000] [12] [INFO] Booting worker with pid: 12\n",
      "[2021-03-19 09:37:42 +0000] [13] [INFO] Booting worker with pid: 13\n",
      "[2021-03-19 09:37:42,700] WARNING - Saved BentoService Python version mismatch: loading BentoService bundle created with Python version 3.7.8, but current environment version is 3.7.6.\n",
      "[2021-03-19 09:37:42,771] WARNING - Saved BentoService Python version mismatch: loading BentoService bundle created with Python version 3.7.8, but current environment version is 3.7.6.\n",
      "[2021-03-19 09:38:35,545] INFO - {'service_name': 'IrisClassifier', 'service_version': '20210319023551_84AAF6', 'api': 'predict', 'task': {'data': '[[5.5, 2.3, 4.0, 1.3], [5.2, 4.1, 1.5, 0.1], [4.4, 2.9, 1.4, 0.2], [5.1, 3.8, 1.9, 0.4], [6.7, 3.3, 5.7, 2.1]]', 'task_id': '3c12e3f8-f7e0-47ed-a055-ed0e62623e6e', 'batch': 5, 'http_headers': (('Host', 'localhost:5000'), ('User-Agent', 'curl/7.71.1'), ('Accept', '*/*'), ('Content-Type', 'application/json'), ('Content-Length', '110'))}, 'result': {'data': '[1, 0, 0, 0, 2]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '3c12e3f8-f7e0-47ed-a055-ed0e62623e6e'}\n",
      "^C\n",
      "[2021-03-19 09:38:49 +0000] [1] [INFO] Handling signal: int\n",
      "[2021-03-19 09:38:49 +0000] [12] [INFO] Worker exiting (pid: 12)\n",
      "[2021-03-19 09:38:49 +0000] [13] [INFO] Worker exiting (pid: 13)\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 5000:5000 iris-classifier:v1 --workers=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso tornou possível implantar modelos de ML agrupados em BentoML com plataformas como\n",
    "[Kubeflow] (https://www.kubeflow.org/docs/components/serving/bentoml/),\n",
    "[Knative] (https://knative.dev/community/samples/serving/machinelearning-python-bentoml/),\n",
    "[Kubernetes] (https://docs.bentoml.org/en/latest/deployment/kubernetes.html), que\n",
    "fornece recursos avançados de implantação de modelo, como escalonamento automático, teste A/B,\n",
    "escala a zero, canary rollout e multi-armed bandit.\n",
    "\n",
    "\n",
    "## Carregar BentoService salvo\n",
    "\n",
    "`bentoml.load` é a API para carregar um modelo empacotado BentoML em python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-19 02:38:54,032] WARNING - Module `iris_classifier` already loaded, using existing imported module.\n",
      "[2021-03-19 02:38:54,071] WARNING - pip package requirement pandas already exist\n",
      "[2021-03-19 02:38:54,072] WARNING - pip package requirement scikit-learn already exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "memmap([1, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "import pandas as pd\n",
    "\n",
    "bento_svc = bentoml.load(saved_path)\n",
    "\n",
    "# Test loaded bentoml service:\n",
    "bento_svc.predict(test_input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O formato BentoML é instalável pip e pode ser distribuído diretamente como um\n",
    "Pacote PyPI para uso em aplicativos Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q {saved_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([1, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The BentoService class name will become packaged name\n",
    "import IrisClassifier\n",
    "\n",
    "installed_svc = IrisClassifier.load()\n",
    "installed_svc.predict(test_input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso também permite que os usuários carreguem seu BentoService para pypi.org como um pacote python público\n",
    "ou para o índice PyPi privado de sua organização para compartilhar com outros desenvolvedores.\n",
    "\n",
    "`cd {saved_path} e python setup.py sdist upload`\n",
    "\n",
    "* Você terá que configurar o arquivo \".pypirc\" antes de enviar para o índice pypi.\n",
    "     Você pode encontrar mais informações sobre como distribuir o pacote python em:\n",
    "     https://docs.python.org/3.7/distributing/index.html#distributing-index*\n",
    "\n",
    "\n",
    "# Lançar trabalho de inferência do CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML cli suporta o carregamento e a execução de um modelo empacotado da CLI. Com o adaptador `DataframeInput`, o comando CLI suporta a leitura de dados Dataframe de entrada do argumento CLI ou arquivos locais` csv` ou `json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 2]\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml run IrisClassifier:latest predict --input '{test_input_df.to_json()}' --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 2]\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml run IrisClassifier:latest predict \\\n",
    "    --input-file \"./test_input.csv\" --format \"csv\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 2]\r\n"
     ]
    }
   ],
   "source": [
    "# run inference with the docker image built above\n",
    "!docker run -v $(PWD):/tmp iris-classifier:v1 \\\n",
    "        bentoml run /bento predict --input-file \"/tmp/test_input.csv\" --format \"csv\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opções de implantação\n",
    "\n",
    "Confira o [guia de implantação do BentoML](https://docs.bentoml.org/en/latest/deployment/index.html)\n",
    "para entender melhor qual opção de implantação é mais adequada para seu caso de uso.\n",
    "\n",
    "* Implantação de um clique com BentoML:\n",
    "  - [AWS Lambda](https://docs.bentoml.org/en/latest/deployment/aws_lambda.html)\n",
    "  - [AWS SageMaker](https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html)\n",
    "  - [AWS EC2](https://docs.bentoml.org/en/latest/deployment/aws_ec2.html)\n",
    "  - [Azure Functions](https://docs.bentoml.org/en/latest/deployment/azure_functions.html)\n",
    "\n",
    "* Implante com plataformas de código aberto:\n",
    "  - [Docker](https://docs.bentoml.org/en/latest/deployment/docker.html)\n",
    "  - [Kubernetes](https://docs.bentoml.org/en/latest/deployment/kubernetes.html)\n",
    "  - [Knative](https://docs.bentoml.org/en/latest/deployment/knative.html)\n",
    "  - [Kubeflow](https://docs.bentoml.org/en/latest/deployment/kubeflow.html)\n",
    "  - [KFServing](https://docs.bentoml.org/en/latest/deployment/kfserving.html)\n",
    "  - [Clipper](https://docs.bentoml.org/en/latest/deployment/clipper.html)\n",
    "\n",
    "* Guias de implantação manual na nuvem:\n",
    "  - [AWS ECS](https://docs.bentoml.org/en/latest/deployment/aws_ecs.html)\n",
    "  - [Google Cloud Run](https://docs.bentoml.org/en/latest/deployment/google_cloud_run.html)\n",
    "  - [Azure container instance](https://docs.bentoml.org/en/latest/deployment/azure_container_instance.html)\n",
    "  - [Heroku](https://docs.bentoml.org/en/latest/deployment/heroku.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo\n",
    "\n",
    "Isso é o que parece quando se usa BentoML para servir e implantar um modelo na nuvem. BentoML também suporta [muitos outros frameworks de aprendizado de máquina](https://docs.bentoml.org/en/latest/examples.html) além de Scikit-learn. O documento [conceitos básicos do BentoML](https://docs.bentoml.org/en/latest/concepts.html) é recomendado para quem deseja obter uma compreensão mais profunda do BentoML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
